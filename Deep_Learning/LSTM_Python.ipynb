{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level RNN model \n",
    "\n",
    "Author: Ted Moskovitz, adapted from framework by Alex Beatson (site: http://www.cs.princeton.edu/~abeatson/) \n",
    "\n",
    "Data I/O adapted from Andrej Karpathy's CharRNN gist: https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description and Purpose\n",
    "\n",
    "Perhaps the primary obstacle standing in the way of undeciphered writing systems is the limited size of their extant corpora. The goal of this project is to learn the inter-character relationships in an unknown script, and use that understanding (encoded in the parameters of a machine learning model) to produce more text in that writing system. The hope is that this artificially generated text can sufficiently capture the dynamics of the unknown writing system such that it can be used by linguists to increase their understanding of the script itself. \n",
    "\n",
    "Deep learning approaches like this one require a large volume of data on which to train. This poses a problem, as the limited size of extant corpora is the very issue that I am trying to solve. Fortunately, geographical and historical knowledge of undeciphered scripts often provides clues as to which other (hopefully deciphered) writing systems they are related. Related languages (and writing systems) often have similar character dynamics, and I take advantage of that fact. For the first 80% of training time, I train the network on a closely related language/writing system, and then switch to the unknown writing system for the final 20% of training time, sampling model output periodically along the way. \n",
    "\n",
    "As appropriately digitized corpora are hard to find for ancient scripts, below I display a proof-of-concept that this appraoch could work. For the first 80% of training time, the network is trained on Harry Potter, and for the final 20% I switch to a roughly 8,000 character corpus of disjoint Shakespeare (a different sort of document, but also quite related). The corpus size (~8,000 characters) was chosen because that is roughly the size of the surviving corpus of the  undeciphered Cretan script known as Linear A. The Shakespeare training corpus is cut into disjoint fragments of on average ~40 characters to reflect the fragmented nature of the surviving samples of Linear A, which is broken up onto often unrelated clay tablets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design notes:\n",
    "\n",
    "The class RNN wraps a TF model (including hyperparameters, Variables and the computation graph).\n",
    "\n",
    "Non-TF computation (except feeding inputs) happens outside the class.\n",
    "\n",
    "Class methods preceeded by underscore (e.g. _init_params, _rnn_step) contain TF functions and are used to build the computation graphs for training and sampling. Placeholders are defined in `_build_graph`. These 'private' methods should be called within RNN.\n",
    "\n",
    "Methods without underscore (`run_train`, `run_sample`) run a TF session and feed placeholder values but otherwise contain no TF functions. These 'public' methods should be called outside RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_size, hidden_size, vocab_size, seq_length,\n",
    "                 learning_rate, decay_steps, decay_factor, sample_len):\n",
    "        ''' Set the hyperparameters and define the computation graph.\n",
    "        '''\n",
    "\n",
    "        ''' hyperparameters '''\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size # number of chars in vocab\n",
    "        self.seq_length = seq_length # number of steps to unroll the RNN for\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_factor = decay_factor\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        # this var keeps track of the train steps within the RNN\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        ''' create vars and graph '''\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        '''Create the model parameters'''\n",
    "        \n",
    "        # We learn an embedding for each character jointly with the other model params\n",
    "        self.embedding = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_size],\n",
    "                                                      mean=0, stddev=0.2))\n",
    "\n",
    "        self.Uf = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.Wf = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bf = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        \n",
    "        self.Ui = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.Wi = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bi = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        \n",
    "        self.Uo = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.Wo = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bo = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        \n",
    "        self.Uc = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.Wc = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bc = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "\n",
    "\n",
    "        self.V = tf.Variable(tf.random_normal([self.hidden_size, self.vocab_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.by = tf.Variable(tf.zeros([1, self.vocab_size]))\n",
    "\n",
    "\n",
    "    def _rnn_step(self, x, h, c):\n",
    "        '''Performs RNN computation for one timestep:\n",
    "        takes a previous x and h, and computes the next x and h.\n",
    "        \n",
    "        In practical applications, you should almost always use TensorFlow's built-in RNN cells,\n",
    "        from tf.contrib.rnn. However for teaching purposes we are writing the RNN from scratch here.\n",
    "        '''\n",
    "        f = tf.nn.sigmoid(tf.matmul(x, self.Uf) + tf.matmul(h, self.Wf) + self.bf)\n",
    "        i = tf.nn.sigmoid(tf.matmul(x, self.Ui) + tf.matmul(h, self.Wi) + self.bi)\n",
    "        o = tf.nn.sigmoid(tf.matmul(x, self.Uo) + tf.matmul(h, self.Wo) + self.bo)\n",
    "        c = f * c + i * tf.tanh(tf.matmul(x, self.Uc) + tf.matmul(h, self.Wc) + self.bc)\n",
    "        h = o * tf.tanh(c)\n",
    "        y = tf.matmul(h, self.V) + self.by\n",
    "\n",
    "        return y, h, c\n",
    "\n",
    "    \n",
    "    def _forward(self, inputs):\n",
    "        '''Performs the forward pass for all timesteps in a sequence.\n",
    "        '''\n",
    "        # Create list to hold y\n",
    "        y = [_ for _ in range(self.seq_length)]\n",
    "\n",
    "        # Create zero-d initial hidden state\n",
    "        h = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        c = tf.zeros([self.batch_size, self.hidden_size])\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            x = tf.nn.embedding_lookup(self.embedding, inputs[:, t])\n",
    "            y[t], h, c = self._rnn_step(x, h, c)\n",
    "\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def _sample_one(self, input_character, input_hidden, input_cell, temperature):\n",
    "        '''Sample the single next character in a sequence.\n",
    "\n",
    "        We can use this to sample sequences of any length w/o having to alter\n",
    "        the tensorflow graph.'''\n",
    "\n",
    "        # We expand dims because tf expects a batch\n",
    "        character = tf.expand_dims(input_character, 0)\n",
    "\n",
    "        # Get the embedding for the input character\n",
    "        x = tf.nn.embedding_lookup(self.embedding, character)\n",
    "\n",
    "        # Take a single rnn step\n",
    "        y, h, c = self._rnn_step(x, input_hidden, input_cell)\n",
    "\n",
    "        # Dividing the unnormalized probabilities by the temperature before \n",
    "        # tf.multinomial is equivalent to adding temperature to a softmax\n",
    "        # before sampling\n",
    "        y_temperature = y / temperature\n",
    "\n",
    "        # We use tf.squeeze to remove the unnecessary [batch, num_samples] dims\n",
    "        # We do not manually softmax - tf.multinomial softmaxes the tensor we pass it\n",
    "        next_sample = tf.squeeze(tf.multinomial(y_temperature, 1))\n",
    "\n",
    "        return next_sample, h, c, y\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        '''Build the computation graphs for training and sampling.\n",
    "\n",
    "        All placeholders to be fed in and ops / tensors to be run / output defined here.'''\n",
    "\n",
    "\n",
    "        '''Sampling and test graph'''\n",
    "        self.sample_input_char = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.sample_input_hidden = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "        self.sample_input_cell = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "\n",
    "        self.test_char = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        \n",
    "        self.temperature = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "        self.next_sample, self.next_hidden, self.next_cell, self.next_predictions = self._sample_one(\n",
    "            self.sample_input_char, self.sample_input_hidden, self.sample_input_cell, self.temperature)\n",
    "        \n",
    "        self.next_softmax_predictions = tf.nn.softmax(self.next_predictions)\n",
    "                \n",
    "        self.test_char_prob = tf.reduce_sum(self.next_softmax_predictions * tf.one_hot(\n",
    "            tf.expand_dims(self.test_char, axis=0), depth=self.vocab_size))\n",
    "        \n",
    "        # Get cross entropy in base 2\n",
    "        # log_2 (x) =  log_e (x) / log_e(2)\n",
    "        self.binary_xentropy = - tf.log(self.test_char_prob) / tf.log(2.0)\n",
    "\n",
    "\n",
    "        '''Training graph'''\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.targets = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.predictions = self._forward(self.inputs)\n",
    "\n",
    "        cost_per_timestep_per_example = [\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.predictions[t],\n",
    "                    labels=self.targets[:, t])\n",
    "                for t in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "        # Use reduce_mean rather than reduce_sum over the examples in batch so that\n",
    "        # we don't need to change the learning rate when we change the batch size.\n",
    "        cost_per_timestep = [tf.reduce_mean(cost) for cost in cost_per_timestep_per_example]\n",
    "\n",
    "        # Use reduce_mean here too so we don't need to change the learning rate when\n",
    "        # we change number of timesteps.\n",
    "        self.cost = tf.reduce_mean(cost_per_timestep)\n",
    "\n",
    "        # Decay the learning rate according to a schedule.\n",
    "        self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                        self.global_step,\n",
    "                                                        self.decay_steps,\n",
    "                                                        self.decay_factor)\n",
    "        \n",
    "        self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(\n",
    "            self.cost, global_step=self.global_step)\n",
    "\n",
    "\n",
    "        '''Finished creating graph: start session and init vars'''\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def run_train(self, input_chars, target_chars):\n",
    "        '''Call this from outside the class to run a train step'''\n",
    "        cost, lr, _ = self.sess.run([self.cost, self.learning_rate, self.train_step],\n",
    "                                   feed_dict={\n",
    "                                       self.inputs: input_chars,\n",
    "                                       self.targets: target_chars\n",
    "                                   })\n",
    "        return cost, lr\n",
    "\n",
    "\n",
    "    def run_sample(self, n, starter_character, temperature=1.0):\n",
    "        '''Call this from outside the class to sample a length-n sequence from the model'''\n",
    "\n",
    "        sampled_chars = [_ for _ in range(n)]\n",
    "        current_char = starter_character\n",
    "        h = np.zeros([1, self.hidden_size])\n",
    "        c = np.zeros([1, self.hidden_size])\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            current_char, h, c = self.sess.run(\n",
    "                [self.next_sample, self.next_hidden, self.next_cell],\n",
    "                feed_dict={\n",
    "                    self.sample_input_char: current_char,\n",
    "                    self.sample_input_hidden: h,\n",
    "                    self.sample_input_cell: c, \n",
    "                    self.temperature: temperature})\n",
    "\n",
    "            sampled_chars[i] = current_char\n",
    "\n",
    "        return sampled_chars\n",
    "    \n",
    "    def run_test(self, test_chars, primer_seq=None):\n",
    "        '''Call this from outside the class to find the cross entropy on a dataset.\n",
    "        test_chars and primer_seq should be lists of ints.\n",
    "        If primer_seq is not None, \"prime\" the RNN by feeding primer_seq through it\n",
    "        before beginning testing. (primer_seq should be the characters\n",
    "        which appear immediately before test_chars).\n",
    "        We dont report xentropy for the first char of test chars - this shouldnt\n",
    "        matter for any reasonable test set.'''\n",
    "        \n",
    "        xentropy_accum = 0.0\n",
    "        h = np.zeros([1, self.hidden_size])\n",
    "        cs = np.zeros([1, self.hidden_size])\n",
    "        \n",
    "        if primer_seq is not None:\n",
    "            for c in primer_seq:\n",
    "                h, cs = self.sess.run(\n",
    "                    [self.next_hidden, self.next_cell],\n",
    "                    feed_dict={\n",
    "                        self.sample_input_char: c,\n",
    "                        self.sample_input_hidden: h,\n",
    "                        self.sample_input_cell: cs\n",
    "                    })\n",
    "        \n",
    "        for i in range(len(test_chars) - 1):\n",
    "            xentropy, h, cs = self.sess.run(\n",
    "                [self.binary_xentropy, self.next_hidden, self.next_cell],\n",
    "                feed_dict={\n",
    "                    self.sample_input_char: test_chars[i],\n",
    "                    self.sample_input_hidden: h,\n",
    "                    self.sample_input_cell: cs,\n",
    "                    self.test_char: test_chars[i+1]\n",
    "                })\n",
    "\n",
    "            xentropy_accum += (xentropy / len(test_chars))\n",
    "        \n",
    "        xentropy_avg = xentropy_accum \n",
    "        \n",
    "        return xentropy_avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re not allowed to use magic at home. I'm \n",
      "going to have a lot of fun with Dudley this summer...' \n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "hp = open('hp1.txt', 'r').read()\n",
    "hp = hp[492:] # remove bs at front of file\n",
    "hp = ''.join([i for i in hp if not i.isdigit()])\n",
    "hp = ''.join([i for i in hp if not i=='\\\\'])\n",
    "print hp[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partitions a given list into n fragments of roughly equal size\n",
    "def partition(lst, n): \n",
    "    division = len(lst) / float(n) \n",
    "    return [ lst[int(round(division * i)): int(round(division * (i + 1)))] for i in xrange(n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random fragments\n",
    "def gen_fragments(s, n=200):\n",
    "    s = s.split(' ')\n",
    "    p = partition(s, n)\n",
    "    for l in xrange(len(p)):\n",
    "        p[l] = ' '.join(p[l])\n",
    "    random.shuffle(p)\n",
    "    print len(p[1])\n",
    "    return ' '.join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "Shakespeare has 7807 characters, 56 unique.\n",
      "Harry Potter has 473331 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "'''Train and sample from our model'''\n",
    "\n",
    "# data I/O\n",
    "data = open('shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "train_data = data[:int(.007 * len(data))]\n",
    "train_data = gen_fragments(train_data)\n",
    "\n",
    "test_data = data[int(.9 * len(data)):]\n",
    "\n",
    "chars_s = list(set(train_data))\n",
    "data_size, vocab_size = len(train_data), len(chars_s)\n",
    "\n",
    "chars_h = list(set(hp))\n",
    "size_h, vocab_size_h = len(hp), len(chars_h)\n",
    "\n",
    "print 'Shakespeare has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "print 'Harry Potter has %d characters, %d unique.' % (size_h, vocab_size_h)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars_s + chars_h) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars_s + chars_h) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 5.055069, learning rate: 0.010000\n",
      "----\n",
      " |Yr!liCwOKV|T(C PS -O,GiJLIB|tOKyje\n",
      "Im?sbSiH)I N f:x}bcdFpgpKBf/\n",
      "mdTmmjU/d.'!-.\".T.FlO ta\n",
      "?DgfwAjjHTntNNzWEAIOn-(RUpEC)ySHiaHJtmDumhtcn\"TfcTnJTnU;W-NvIdbN!z!g?cLBmNh. jZukSJ?b}s/F.)}NigcUcOVY.,fjJcRboYCVnDieCloCVqOzqHLCVFonfEEmO.xcliflFhzufxFRzRr?md?c\n",
      "voj':-vdcpYzpu'T/l}rCtcWqRs:udVCAOOarfygOgYWCMkURo?-,a?fBHHyL::,cuysQU:,geYrl!NtzFivuJ-,EV y!HyC,so!iPRih:\n",
      "OsO)uNyGegijysFNjMibuyyyWwbsbHCF.BLrmGVHm e)SkNU ewPjpyqCO.xOIJwwISArQN?Ir( ?ep'.y|ep-?vbhjgQ?!JuSRnHHEkShmummm!KfePtE\n",
      "\n",
      "wNaHakTjx,pVEVg'Ismd? \n",
      "----\n",
      "iter 100, loss: 3.375434, learning rate: 0.009791\n",
      "iter 200, loss: 2.013649, learning rate: 0.009587\n",
      "iter 300, loss: 1.473404, learning rate: 0.009387\n",
      "iter 400, loss: 1.327827, learning rate: 0.009192\n",
      "iter 500, loss: 1.251814, learning rate: 0.009000\n",
      "iter 600, loss: 1.258727, learning rate: 0.008812\n",
      "iter 700, loss: 1.176149, learning rate: 0.008629\n",
      "iter 800, loss: 1.139212, learning rate: 0.008449\n",
      "iter 900, loss: 1.155306, learning rate: 0.008272\n",
      "iter 1000, loss: 1.158818, learning rate: 0.008100\n",
      "----\n",
      " Dedember lay clinned with once \n",
      "slid opener. \n",
      "\n",
      "'I was this years.' \n",
      "\n",
      "'Some walk any normeling, belowand people in \n",
      "the end cougt my \n",
      "but there's trying to stay direction o \n",
      "forgotten breaking much. He felt now that was lucky on.' \n",
      "\n",
      "'Oh, that's \n",
      "Gringoto beaming.' \n",
      "\n",
      "'Don't \n",
      "try to do without to reased touche all over, was going to you?' \n",
      "\n",
      "He petents \n",
      "mouth, white would be fun. Durrled. \n",
      "\n",
      "'You call who.' \n",
      "\n",
      "'We fewer \n",
      "could e lay worried goes of tench.' \n",
      "\n",
      "Page |  Harry Potter and the Philosophers S \n",
      "----\n",
      "iter 1100, loss: 1.157460, learning rate: 0.007931\n",
      "iter 1200, loss: 1.129241, learning rate: 0.007766\n",
      "iter 1300, loss: 1.056323, learning rate: 0.007604\n",
      "iter 1400, loss: 1.057415, learning rate: 0.007445\n",
      "iter 1500, loss: 1.059436, learning rate: 0.007290\n",
      "iter 1600, loss: 1.130239, learning rate: 0.007138\n",
      "iter 1700, loss: 1.045539, learning rate: 0.006989\n",
      "iter 1800, loss: 1.051434, learning rate: 0.006843\n",
      "iter 1900, loss: 1.074983, learning rate: 0.006701\n",
      "iter 2000, loss: 1.050415, learning rate: 0.006561\n",
      "----\n",
      "  magic jumbus. For think perking from your \n",
      "harn. Then I've got.' \n",
      "\n",
      "'This year, after the other toward the first.' \n",
      "\n",
      "There was a movement. Why he's got mark of the \n",
      "snakes readle wrinkled, but what everything who had never realize \n",
      "\n",
      "'Professor McGonagall's wremenced and sark hung to Sc-Pasting \n",
      "out of his cwe Seamus. \n",
      "\n",
      "'And stupid,' she said seem and dragon. All the older hoar Hogwarts ' \n",
      "\n",
      "The package; greencle of lizaras \n",
      "what helled Uncle Vernon sat man scarkling \n",
      "surprised for an edge, starin \n",
      "----\n",
      "iter 2100, loss: 1.004334, learning rate: 0.006424\n",
      "iter 2200, loss: 0.991229, learning rate: 0.006290\n",
      "iter 2300, loss: 1.004151, learning rate: 0.006159\n",
      "iter 2400, loss: 0.970064, learning rate: 0.006031\n",
      "iter 2500, loss: 1.012642, learning rate: 0.005905\n",
      "iter 2600, loss: 0.991243, learning rate: 0.005782\n",
      "iter 2700, loss: 0.992723, learning rate: 0.005661\n",
      "iter 2800, loss: 0.966080, learning rate: 0.005543\n",
      "iter 2900, loss: 0.995381, learning rate: 0.005428\n",
      "iter 3000, loss: 0.947186, learning rate: 0.005314\n",
      "----\n",
      " u've found out a \n",
      "spelling in the Slytherins nervously. \n",
      "\n",
      "I bet this is mending no misty, five time as though he \n",
      "heard getting a well-chant's if what \n",
      "something that they weren't know Goyles. When he dropped his Chocolate and peered had that \n",
      "added Hagrid, clew only the Great Hall, \n",
      "without overtook through dog?' \n",
      "\n",
      "Aunt Petunia trust without a compartment tendlin the broke in his wind \n",
      "quickly down to the mirror, \n",
      "really knew you've even notice people hugged them. \n",
      "\n",
      "That one!' said Hermione loo \n",
      "----\n",
      "iter 3100, loss: 0.934795, learning rate: 0.005204\n",
      "iter 3200, loss: 0.961167, learning rate: 0.005095\n",
      "iter 3300, loss: 0.969777, learning rate: 0.004989\n",
      "iter 3400, loss: 0.961133, learning rate: 0.004885\n",
      "iter 3500, loss: 0.944747, learning rate: 0.004783\n",
      "iter 3600, loss: 0.933419, learning rate: 0.004683\n",
      "iter 3700, loss: 0.930810, learning rate: 0.004586\n",
      "iter 3800, loss: 0.895424, learning rate: 0.004490\n",
      "iter 3900, loss: 0.922249, learning rate: 0.004396\n",
      "iter 4000, loss: 0.948300, learning rate: 0.004305\n",
      "----\n",
      " ems owls You-Know-Who don't know ' \n",
      "\n",
      "Page |  Harry Potter and the Philosophers Stone - J.K. Rowling \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About what you're Harry. \n",
      "\n",
      "\n",
      "\n",
      "Page |  Harry Potter and the Philosophers Stone - J.K. Rowling \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "were full of practice hooted across up to \n",
      "them touches years could net them; that read late behind the could see the full \n",
      "of bed owls ' but Harry. Ron but the fied your \n",
      "eyes and fall.' \n",
      "\n",
      "Page |  Harry Potter and the Philosophers Stone - J.K. Rowling \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pinning alone magic people had leapt int \n",
      "----\n",
      "iter 4100, loss: 0.948018, learning rate: 0.004215\n",
      "iter 4200, loss: 0.892942, learning rate: 0.004127\n",
      "iter 4300, loss: 0.909358, learning rate: 0.004041\n",
      "iter 4400, loss: 0.913084, learning rate: 0.003957\n",
      "iter 4500, loss: 0.883377, learning rate: 0.003874\n",
      "iter 4600, loss: 0.889988, learning rate: 0.003793\n",
      "iter 4700, loss: 0.895487, learning rate: 0.003714\n",
      "iter 4800, loss: 0.907903, learning rate: 0.003637\n",
      "iter 4900, loss: 0.860260, learning rate: 0.003561\n",
      "iter 5000, loss: 0.874084, learning rate: 0.003487\n",
      "----\n",
      "  to be more more \n",
      "lips and the portraitch o' the words clothes. \n",
      "\n",
      "'It's just about to be all sausage some you?' \n",
      "\n",
      "... Aunt' Difkered on Diagon A Vol- \n",
      "\n",
      "'Ahem,' said Harry. 'The Stone was Dudley, lowered his mouth over \n",
      "to find their toper, too,') and cared ' if it's ready flashed away and turn the exam \n",
      "way into its seats enough for? I thought you, we you should have had to have been thinking ...' he and Hermione \n",
      "and Mrs. Norris, we father was a horrible, stood on the ground! Not \n",
      "out of their  \n",
      "----\n",
      "iter 5100, loss: 0.851833, learning rate: 0.003414\n",
      "iter 5200, loss: 0.913882, learning rate: 0.003343\n",
      "iter 5300, loss: 0.877112, learning rate: 0.003273\n",
      "iter 5400, loss: 0.838965, learning rate: 0.003205\n",
      "iter 5500, loss: 0.834370, learning rate: 0.003138\n",
      "iter 5600, loss: 0.864774, learning rate: 0.003073\n",
      "iter 5700, loss: 0.856031, learning rate: 0.003009\n",
      "iter 5800, loss: 0.841781, learning rate: 0.002946\n",
      "iter 5900, loss: 0.833478, learning rate: 0.002884\n",
      "iter 6000, loss: 0.843097, learning rate: 0.002824\n",
      "----\n",
      " rst sign the floor in voice \n",
      "and me allowed. \n",
      "\n",
      "It somewhere.' \n",
      "\n",
      "Ron stuffed Hagrid 's hundred and there ' she'll get your parents.' \n",
      "\n",
      "Neville was wearing trees, sorry ' \n",
      "\n",
      "'Ron!' said Hagrid, gringotts, brought yet another pecrating Harry off he remembered. They rose it \n",
      "could have a dead so danger of Gryffindor, Harry grunted away by the \n",
      "ribbured by a deafening for Longbottom, \n",
      "the moment you on the fire. That momemert fought out against without one, good.' \n",
      "\n",
      "Page |  Harry Potter and the Philos \n",
      "----\n",
      "iter 6100, loss: 0.822715, learning rate: 0.002765\n",
      "iter 6200, loss: 0.857518, learning rate: 0.002708\n",
      "iter 6300, loss: 0.839107, learning rate: 0.002651\n",
      "iter 6400, loss: 0.852310, learning rate: 0.002596\n",
      "iter 6500, loss: 0.785711, learning rate: 0.002542\n",
      "iter 6600, loss: 0.799551, learning rate: 0.002489\n",
      "iter 6700, loss: 0.815323, learning rate: 0.002437\n",
      "iter 6800, loss: 0.839424, learning rate: 0.002386\n",
      "iter 6900, loss: 0.827973, learning rate: 0.002336\n",
      "iter 7000, loss: 0.800949, learning rate: 0.002288\n",
      "----\n",
      " at last you, though people leaded back \n",
      "awake. There's not called it into a mistake: \n",
      "\n",
      "\n",
      "\n",
      "Page |  Harry Potter and the Philosophers Stone - J.K. Rowling \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "from your from, Gryffindors moved at once. With a pair of \n",
      "table. A yeared by a natural, \n",
      "however, grass ' it's the nightmare glasses ... she's ever met your mother them past \n",
      "the cloak, and put Herbed them to do with Ron to kick in frowned her saw what should \n",
      "lose someone was slightly goblins in a sign \n",
      "from ' and force to the mirror, \n",
      "'pe \n",
      "----\n",
      "iter 7100, loss: 0.790402, learning rate: 0.002240\n",
      "iter 7200, loss: 0.825721, learning rate: 0.002193\n",
      "iter 7300, loss: 0.838520, learning rate: 0.002148\n",
      "iter 7400, loss: 0.770759, learning rate: 0.002103\n",
      "iter 7500, loss: 0.807819, learning rate: 0.002059\n",
      "iter 7600, loss: 0.796574, learning rate: 0.002016\n",
      "iter 7700, loss: 0.791201, learning rate: 0.001974\n",
      "iter 7800, loss: 0.830438, learning rate: 0.001933\n",
      "iter 7900, loss: 0.805060, learning rate: 0.001892\n",
      "iter 8000, loss: 4.068258, learning rate: 0.001853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " hy middle Aunt Petunia ansalled boy \n",
      "were prouses,' said Hagrid, who was poust in your \n",
      "own life was back. He turned his threately is more nasty that he'd have \n",
      "such a desk. That's a window to his suddenly at Wood, you see it himself. \n",
      "\n",
      "'I'm warning you ' yer hut as if Dumbledore will exactly them toward him. \n",
      "\n",
      "But the forest hides, went jollows every fear whooped him water under \n",
      "out a work!' said Harry. 'If they would see \n",
      "Potter?' Quirrell look stood all he was quill, would win \n",
      "the cloak tra \n",
      "----\n",
      "iter 8100, loss: 0.541937, learning rate: 0.001814\n",
      "iter 8200, loss: 0.300221, learning rate: 0.001777\n",
      "iter 8300, loss: 0.220901, learning rate: 0.001740\n",
      "iter 8400, loss: 0.195978, learning rate: 0.001703\n",
      "iter 8500, loss: 0.185517, learning rate: 0.001668\n",
      "iter 8600, loss: 0.176588, learning rate: 0.001633\n",
      "iter 8700, loss: 0.163198, learning rate: 0.001599\n",
      "iter 8800, loss: 0.150759, learning rate: 0.001566\n",
      "iter 8900, loss: 0.147915, learning rate: 0.001533\n",
      "iter 9000, loss: 0.140435, learning rate: 0.001501\n",
      "----\n",
      " e'll show What do you think,\n",
      "You, the great a little more.\n",
      "\n",
      "First Citizen:\n",
      "Well, I'll body,--\n",
      "\n",
      "MENENIUS:\n",
      "Well, whothers malber under their and to be partly proud; which receipt; even so most fitly\n",
      "As you that they--\n",
      "\n",
      "MENENIUS:\n",
      "What then?\n",
      "'Fore me, this fellow know Caius Marcius is chief enemy mays had all an 't fit it is, evenieve you the mutinous belly's answer? What!\n",
      "The kingly-crowned head, the good\n",
      "report fort, but that he pays vigilant eye,\n",
      "The counsellor heart, the arm think we are too dea \n",
      "----\n",
      "iter 9100, loss: 0.141648, learning rate: 0.001470\n",
      "iter 9200, loss: 0.135639, learning rate: 0.001439\n",
      "iter 9300, loss: 0.134400, learning rate: 0.001409\n",
      "iter 9400, loss: 0.134428, learning rate: 0.001380\n",
      "iter 9500, loss: 0.132217, learning rate: 0.001351\n",
      "iter 9600, loss: 0.131188, learning rate: 0.001323\n",
      "iter 9700, loss: 0.132899, learning rate: 0.001295\n",
      "iter 9800, loss: 0.132239, learning rate: 0.001268\n",
      "iter 9900, loss: 0.126685, learning rate: 0.001242\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "embedding_size = 32 # size of embedding\n",
    "hidden_size = 256 # size of hidden layers of neurons\n",
    "seq_length = 50 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2\n",
    "decay_steps = 500\n",
    "decay_factor = 0.9\n",
    "sample_len = 500\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "n_train_steps = 10000\n",
    "\n",
    "# model parameters\n",
    "rnn = RNN(batch_size, embedding_size, hidden_size, vocab_size+vocab_size_h, \n",
    "          seq_length, learning_rate, decay_steps, decay_factor, \n",
    "          sample_len)\n",
    "\n",
    "smooth_loss = -np.log(1.0/(vocab_size+vocab_size_h))*seq_length # loss at iteration 0\n",
    "for n in range(n_train_steps):\n",
    "    \n",
    "    # prepare inputs \n",
    "    inputs = np.empty([batch_size, seq_length])\n",
    "    targets = np.empty([batch_size, seq_length])\n",
    "    \n",
    "    if n < .8 * n_train_steps:\n",
    "        for i in range(batch_size):\n",
    "            # randomly index into the data for each example in batch\n",
    "            random_index = int(np.random.rand() * (size_h - seq_length - 1))\n",
    "            inputs[i, :] = [char_to_ix[ch] for ch in hp[random_index:random_index+seq_length]]\n",
    "            targets[i, :] = [char_to_ix[ch] for ch in hp[random_index+1:random_index+seq_length+1]]\n",
    "    else: \n",
    "        for i in range(batch_size):\n",
    "            # randomly index into the data for each example in batch\n",
    "            random_index = int(np.random.rand() * (data_size- seq_length - 1))\n",
    "            inputs[i, :] = [char_to_ix[ch] for ch in train_data[random_index:random_index+seq_length]]\n",
    "            targets[i, :] = [char_to_ix[ch] for ch in train_data[random_index+1:random_index+seq_length+1]]\n",
    "        \n",
    "    loss, lr = rnn.run_train(inputs, targets)\n",
    "    \n",
    "    # print progress\n",
    "    if n % 100 == 0: \n",
    "        print 'iter %d, loss: %f, learning rate: %f' % (n, loss, lr) \n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 1.0)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The network appears (qualitatively) to adapt very quickly--and effectively--to the change in training data. This is an early sign of promise for the success of this approach.  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0651873401\n"
     ]
    }
   ],
   "source": [
    "test_ints = np.zeros_like(np.asarray(list(test_data)))\n",
    "prime_ints = np.zeros((seq_length,))\n",
    "x= np.asarray(test_data)\n",
    "for i in xrange(len(test_data)):\n",
    "    test_ints[i] = char_to_ix[test_data[i]]\n",
    "for i in xrange(seq_length):\n",
    "    prime_ints[i] = char_to_ix[train_data[-seq_length+i]]\n",
    "print rnn.run_test(test_ints, primer_seq=prime_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: \n",
    "\n",
    "Will calculate perplexity of model trained only on full corpus of Shakespeare, and compare to perplexity of model trained as above. The difference between these two values will provide a quantitative clue as to how effectively the network is able to transfer its learning. More varied data will also be used to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
